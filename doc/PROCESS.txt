==================================================================================
 The Tokyo Project is hosted on Sourceforge:
 http://sourceforge.net/projects/tokyo/
 
 Copyright (c) 2005-2008 Eric Bréchemier
 http://eric.brechemier.name
 Licensed under BSD License and/or MIT License.
 See: http://creativecommons.org/licenses/BSD/
==================================================================================

==========================================
Title: Tokyo Project - Process Definition
Last modified: 2008-01-08
By: Eric Bréchemier
Encoding: UTF-8
==========================================

                        *******************************   
                        **       Tokyo Project       **
                        **     Process Definition    **
                        *******************************


* OBJECTIVES
- provide an XML view of non-XML data
- let applications view and create non-XML documents as easily as XML ones
- transformation of non-XML documents to XML and back again

* VISION
- we start with:
    - on the one hand, lots of existing applications working on xml data,
      lots of developers with experience in xml processing,
    - on the other hand, many remaining non-xml formats
      (e.g. for pure binary or pure text data)

- handling non-XML data as XML requires two transformations:
    - from non-xml to xml, for applications input,
    corresponding to XML parsing
    - from xml to non-xml, for applications output,
    corresponding to XML serialization

- an XML parser works on standard XML files/data and implements some XML Parser API (DOM, SAX,...)
  Applications do not access XML data directly, but through the provided API
  
- an XML serializer implements some XML Serializer API (DOM,XML Pull,StAX...).
  Applications often do not create XML data directly, but through the provided API.
  Some applications create XML data directly - this data will have to be converted in a second step.
    
- the structure of an XML document is explicit, it uses the same syntax for all XML documents,
  no matter which grammar/vocabulary they use. Parsers can process this data without any knowledge
  of the document semantics.
  Structure in non-XML data is often implicit or uses a very specific syntax.
  Applications often need to know some grammar to be able to parse the data.

* A SAMPLE SIMPLE PROCESS
Enough concepts! Let me introduce a simple sample process, or a sample simple process if you prefer,
applied to the first prototype: How to sort CSV records using XSLT.

We will grow a working prototype step by step, using an iterative process. We will start with a basic
chain of two TokyoNauts performing a file copy by transferring buffers from a Source to a Destination,
then insert more TokyoNauts in pairs to handle Text encoding, CSV, SAX, and one more playing the role
of a Bridge for XSLT Transformation.

You can refer to the files of this example in the folder "prototype-1-csv2xml" of Tokyo Project download.

===================================================================================================
                      =#= Stage 1: Skeleton for the main loop =#=
===================================================================================================
                      
The first step will be to write the main loop, which we call ProtoOneMainLoop because it is the 
Prototype One, and it is the main loop. This class has a single method main(), the entry point 
for the complete application.

  package net.sf.tokyo.prototype1;
  
  public class ProtoOneMainLoop
  {
    public static void main(String[] args)
    {
      
    } 
  }

We can run this prototype, and it does nothing. Stage 1: Clear.

===================================================================================================
                      =#= Stage 2: First main loop, Source to Destination =#=
===================================================================================================

For the first version of the main loop, we will start with two TokyoNauts:
  - a Source reading bytes from a file
  - a Destination writing bytes to a file

We'll need:
  - code to set up the chain of two TokyoNauts,
  - main loop code to run the processing
  - code to unchain the TokyoNauts and free ressources
  - one class for the first TokyoNaut: FileToBinaryNaut (reading file into buffers of binary data)
  - one class for the last TokyoNaut: BinaryToFileNaut (writing file from buffers of binary data)

Let's go! As a starter, I copy and paste the code sample for the main loop featured in the 
ITokyoNaut#areWeThereYet() javadoc:

  public class ProtoOneMainLoop
  {
    public static void main(String[] args)
    {
      int[] meta = new int[] 
        {
          ITokyoNaut.VERSION_NANA, 
          ITokyoNaut.LANGUAGE_BINARY, ITokyoNaut.TOKEN_SPARK,
          ITokyoNaut.LEFT_START, 0, 0, ITokyoNaut.RIGHT_END
        };
      byte[] data = new byte[10];
      
      int step = 0;
      final int STEP_LIMIT = 99;
      
      while(  (step++ < STEP_LIMIT) && !destination.areWeThereYet(meta,data)  )
      {
        if ( meta[ITokyoNaut.VERSION] == ITokyoNaut.VERSION_NANA)
        {
          System.out.println
            (  
               "Step: "+step+"\n\t"
              +"Language: "+meta[ITokyoNaut.LANGUAGE]+"\n\t"
              +"Token: "+meta[ITokyoNaut.TOKEN]+"\n\t"
              +"Left Relation: "+(meta[ITokyoNaut.LEFT]==1?"+":"")+meta[ITokyoNaut.LEFT]+"\n\t"
              +"Fragment Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
              +"Fragment Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
              +"Right Relation: "+meta[ITokyoNaut.RIGHT]+"\n"
            );
        }
      }
    } 
  }
  
I now get an error at compiling because destination is not defined:
  [javac] (...)/tokyo/trunk/prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/ProtoOneMainLoop.java:58: 
          cannot find symbol
  [javac] symbol  : variable destination
  [javac] location: class net.sf.tokyo.prototype1.ProtoOneMainLoop
  [javac]     while(  (step++ < STEP_LIMIT) && !destination.areWeThereYet(meta,data)  )

This is because we still have no TokyoNaut and destination has not been initialized.
To pass this step, we will create two TokyoNaut classes with a null implementation, import them here,
initialize them and recompile...

I know of a Null TokyoNaut implementation, part of the main package (in the "test" section).
It is not part of the Tokyo API, but I can copy and paste this file as a template for my two new TokyoNauts.
So I copy main/test/java/net/sf/tokyo/test/NullTokyoNaut.java
to prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/tokyonauts/FileToBinaryNaut.java 
and prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/tokyonauts/BinaryToFileNaut.java
then change the class name and package name to match the file and location.

Here is what the FileToBinaryNaut looks like:

  package net.sf.tokyo.prototype1.tokyonauts;
  
  import net.sf.tokyo.ITokyoNaut;
  
  /**
   * Null implementation of ITokyoNaut interface.<br/>
   *
   * <p>
   * Does nothing and if used without boundary will run in a main loop forever.
   * This source code can be used as a base for TokyoNaut implementations.
   * </p>
   */
  public class FileToBinaryNaut implements ITokyoNaut
  {
    public boolean areWeThereYet(int[] meta, byte[] data)
    {
      return false;
    }
    
    public ITokyoNaut plug(ITokyoNaut friend)
    {
      return friend;
    }
    
    public ITokyoNaut unplug(ITokyoNaut foe)
    {
      return foe;
    }
  }

I now import the two TokyoNauts in ProtoOneMainLoop, initialize two instances, source and destination,
plug them together before the main loop, and unplug them after. Here is the result:

  package net.sf.tokyo.prototype1;

  import net.sf.tokyo.ITokyoNaut;
  
  import net.sf.tokyo.prototype1.tokyonauts.FileToBinaryNaut;
  import net.sf.tokyo.prototype1.tokyonauts.BinaryToFileNaut;
  
  public class ProtoOneMainLoop
  {
    public static void main(String[] args)
    {
      FileToBinaryNaut source = new FileToBinaryNaut();
      BinaryToFileNaut destination = new BinaryToFileNaut();
      
      source.plug(destination);
      
      int[] meta = new int[] 
        {
          ITokyoNaut.VERSION_NANA, 
          ITokyoNaut.LANGUAGE_BINARY, ITokyoNaut.TOKEN_SPARK,
          ITokyoNaut.LEFT_START, 0, 0, ITokyoNaut.RIGHT_END
        };
      byte[] data = new byte[10];
      
      int step = 0;
      final int STEP_LIMIT = 99;
      
      while(  (step++ < STEP_LIMIT) && !destination.areWeThereYet(meta,data)  )
      {
        if ( meta[ITokyoNaut.VERSION] == ITokyoNaut.VERSION_NANA)
        {
          System.out.println
            (  
               "Step: "+step+"\n\t"
              +"Language: "+meta[ITokyoNaut.LANGUAGE]+"\n\t"
              +"Token: "+meta[ITokyoNaut.TOKEN]+"\n\t"
              +"Left Relation: "+(meta[ITokyoNaut.LEFT]==1?"+":"")+meta[ITokyoNaut.LEFT]+"\n\t"
              +"Fragment Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
              +"Fragment Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
              +"Right Relation: "+meta[ITokyoNaut.RIGHT]+"\n"
            );
        }
      }
      
      source.unplug(destination);
    } 
  }

It not only compiles again, but also the loop runs correctly for 99 iterations then stops. 
You can say, Stage 2: Complete.

===================================================================================================
                      =#= Stage 3: Two TokyoNauts perform File to File Copy =#=
===================================================================================================

Now it's time to actually perform something useful with our two TokyoNauts. To read one file and
write another, we need the actual locations of the two files. We will add one parameter with the file
location to the constructor of each TokyoNaut, and get these two values from the command line.
To keep a little more freedom, we will actually request only the ouput folder location, and append the 
file name, which will let us create several different files in this folder if needed, e.g. to compare
the output of different tests:

  public static void main(String[] args)
  {
    if (args.length <2)
    {
      System.out.println("Usage: [ProtoOneMain] inFilePath outDirPath");
      return;
    }
    String inFilePath = args[0];
    String outDirPath = args[1];
    String outFilePath = outDirPath + "/result.csv";
    
    System.out.println("Starting ProtoOne:"
      + "\n  In: " + inFilePath
      + "\n  Out: " + outFilePath
    );
    
    FileToBinaryNaut source = new FileToBinaryNaut(inFilePath);
    BinaryToFileNaut destination = new BinaryToFileNaut(outFilePath);
    (...)
  }

After adding the code in FileToBinaryNaut and BinaryToFileNaut with the new constructor (you can leave
it empty for now) everything runs fine with no change.

We now add the code to initialize a FileInputStream in FileToBinaryNaut constructor, and a 
FileOutputStream in BinaryToFileNaut:

  // in FileToBinaryNaut.java,
  protected FileInputStream _in;
  
  public FileToBinaryNaut(String filePath)
  {
    try 
    {
      _in = new FileInputStream(filePath);
    }
    catch(Exception e)
    {
      System.err.println("Error in FileToBinaryNaut(): "+e);
    }
  }
  
  // in BinaryToFileNaut.java,
  protected FileOutputStream _out;
  
  public BinaryToFileNaut(String filePath)
  {
    try 
    {
      _out = new FileOutputStream(filePath);
    }
    catch(Exception e)
    {
      System.err.println("Error in BinaryToFileNaut(): "+e);
    }
  }

Don't forget to add the imports,
  // in FileToBinaryNaut.java,
  import java.io.FileInputStream;
  
  // in BinaryToFileNaut.java,
  import java.io.FileOutputStream;

As you see, the processing is very similar in FileToBinaryNaut and BinaryToFileNaut, and we expect
even more so for the implementation of the methods in the ITokyoNaut interface. Therefore, I will
create a common superclass for the two, called NCommonBase (the "N" should remind you of the "Naut")
and make both classes inherit from it. Since NCommonBase is not a stand-alone class that I want to
create instance from, I will declare it abstract and leave the implementation of missing bits to
the derived classes:
  
  package net.sf.tokyo.prototype1.tokyonauts;

  import net.sf.tokyo.ITokyoNaut;
  
  public abstract class NCommonBase implements ITokyoNaut
  {
  }

I now declare the inheritance:
  // in FileToBinaryNaut.java,
  public class FileToBinaryNaut extends NCommonBase implements ITokyoNaut
  
  // in BinaryToFileNaut.java,
  public class BinaryToFileNaut extends NCommonBase implements ITokyoNaut

Now I add code in NCommonBase, to handle limit conditions in areWeThereYet(). I also declare a
common field _src which will hold the source TokyoNaut set after a call to plug() and unset in 
unplug(). The code in plug() looks like magic because it reflects a dialogue between the two
TokyoNauts (the source and destination) discussing about chicken-and-egg to decide who is the chicken,
er.. the source, and who is the destination. The outcome is eventually that by calling
  
  source.plug(destination)
  
you actually trigger
  
  destination._src = source
  
So here is the complete code of NCommonBase (after lots of errands) including checks 
for limit conditions (quite straightforward based on ITokyoNaut javadoc):

  package net.sf.tokyo.prototype1.tokyonauts;
  
  import net.sf.tokyo.ITokyoNaut;
  
  /**
   * Base implementation of ITokyoNaut interface based on Version NANA.<br/>
   *
   * <p>
   * Abstract class used as a base class for TokyoNaut implementations in Prototype One.
   * </p>
   */
  public abstract class NCommonBase implements ITokyoNaut
  {
    protected ITokyoNaut _src;
    
    protected NCommonBase()
    {
      _src = null;
    }
    
    public boolean areWeThereYet(int[] meta, byte[] data)
    {
      if (meta==null)
        return true;
        
      if 
        (    data==null 
          || meta[OFFSET]<0 || meta[OFFSET]>data.length 
          || meta[OFFSET]+meta[LENGTH]<0 || meta[OFFSET]+meta[LENGTH]>data.length 
        )
      {
        meta[LANGUAGE] = LANGUAGE_ERROR;
        meta[TOKEN] = 0xA00;
        return true;
      }
      
      return false;
    }
    
    public ITokyoNaut plug(ITokyoNaut friend)
    {
      if (friend==null)
        return friend;
      
      // Source or Destination?
      // for now, we don't know whether this is a source or a destination
      // which will be determined by the following ping/pong exchange.
      // This is because, for usability, the call is actually src.plug(dest)
      // while for ease of implementation, the inside wanted relationship is
      // dest._src = src
      
      // Step 1: [@source] if null or same as current source, do nothing
      // Step 4: [@destination] _src is null and the friend param is not, so we continue
      // Step 7: [@source] this time, _src is set, so we stop and return _src
      // 
      if (friend==null || friend==_src )
        return friend;
      
      // Step 2: [@source] set _src to stop the recursion at Step 7,
      //                   original source is saved in "previous", to be restored at step 11.
      // Step 5: [@destination] set _src, which is actually the final desired effect in destination
      ITokyoNaut previous = _src;
      _src = friend;
      
      // A different behavior in source and destination:
      // Due to the sequence,
      // User --> source --> destination
      //                 <--
      //                 ...> (result)
      //        (result) <...
      // destination gets the result before source does, which means
      // that the destination can detect its role because it gets ******** as result.
      
      // Step 3: [@source] Check the reverse associate
        // Step 6: [@destination] Check the reverse associate
        // Step 8: [@destination] result is received, per Step 7, result will be "this"
      // Step 10: [@source] Check the reverse associate, per Step 9, result will be destination "friend" (not "this")
      ITokyoNaut result = friend.plug(this);
      
      if ( result!=this )
      {
        // Step 11: [@source] we received destination, different from self,
        //                    _src is reset to previous to cancel the association process in this direction.
        //                    (the final association is unidirectional destination-->source)
        //                    The destination "friend" is returned to allow chained calls.
        
        _src = previous;
        //System.out.println("Added: association "+this+" <-- "+friend);
        return friend;
      }
      
      // Step 9:  [@destination] we received self, we will return self to confirm it is the actual destination
      return this;
    }
    
    public ITokyoNaut unplug(ITokyoNaut foe)
    {
      if (foe!=null)
        foe.unplug(null);
      
      if (foe==_src)
        _src = null;
      
      return foe;
    }
  }
  
After completing NCommonBase, I can now remove the plug() method from FileToBinaryNaut and 
BinaryToFileNaut altogether (I will stick with the default behavior...) and add finalization code
in unplug() to close the files:

  // in FileToBinaryNaut.java,
  public ITokyoNaut unplug(ITokyoNaut foe)
  {
    if (foe==null || _src == null)
      return foe;
    
    try
    {
      if (_in!=null)
        _in.close();
      _in = null;
    }
    catch(Exception e)
    {
      System.err.println("Error closing file in FileToBinaryNaut() "+e);
    }
    
    return super.unplug(foe);
  }
  
  // in BinaryToFileNaut.java,
  public ITokyoNaut unplug(ITokyoNaut foe)
  {
    try
    {
      if (_out!=null)
        _out.close();
      _out = null;
    }
    catch(Exception e)
    {
      System.err.println("Error closing file in NWriteFile#unplug() "+e);
    }
    
    return super.unplug(foe);
  }
  
Finally, we've just completed the plumbing, and we arrive to the core of each TokyoNaut processing,
within areWeThereYet() method. Here is the code for FileToBinaryNaut
  
  // in FileToBinaryNaut.java,
  public boolean areWeThereYet(int[] meta, byte[] data)
  {
    if ( super.areWeThereYet(meta,data) || _in==null )
      return true;
      
    meta[LANGUAGE] = LANGUAGE_BINARY;
    meta[TOKEN] = TOKEN_BINARY;
    meta[LEFT] = (_isStart? LEFT_START: LEFT_CONTINUED);
    _isStart = false;
    meta[OFFSET] = 0;
    meta[LENGTH] = data.length;
    
    try
    {
      if ( _in.available()==0 )
        return true;
      
      int bytesRead = _in.read(data,meta[OFFSET],meta[LENGTH]);
      meta[LENGTH] = (bytesRead==-1? 0 : bytesRead);
      meta[RIGHT] = (_in.available()==0? RIGHT_END : RIGHT_CONTINUED);
    }
    catch(Exception e)
    {
      System.err.println("Error in FileToBinaryNaut#areWeThereYet(): "+e);
      meta[LANGUAGE]=LANGUAGE_ERROR;
      meta[TOKEN]=0x101;
      return true;
    }
    
    return false;
  }
  
You can notice that this code creates TOKEN_BINARY events, as many as required, the first having
meta[LEFT] = LEFT_START, thanks to the protected member _isStart, which I added to the class, and 
initialized to true in the constructor. The last token has meta[RIGHT] = RIGHT_END.

The code for BinaryToFileNaut is quite similar; the main difference is that BinaryToFileNaut relies 
on _source to provide input data, and terminates when no more data is available:
  
  // in BinaryToFileNaut.java,
  public boolean areWeThereYet(int[] meta, byte[] data)
  {
    if ( super.areWeThereYet(meta,data) || _src==null || _out==null )
      return true;
    
    if ( _src.areWeThereYet(meta,data) )
      return true;
    
    // Skip unknown tokens
    if ( meta[LANGUAGE]!=LANGUAGE_BINARY || meta[TOKEN]!=TOKEN_BINARY )
      return false;
    
    try
    {
      _out.write(data,meta[OFFSET],meta[LENGTH]);
    }
    catch(Exception e)
    {
      System.err.println("Error in BinaryToFileNaut#areWeThereYet(): "+e);
      meta[LANGUAGE]=LANGUAGE_ERROR;
      meta[TOKEN]=0x201;
    }
    
    return false;
  }
  
Now if you followed me till this point, you can check the file "result.csv" created in "out/data",
it is a carbon copy of the input file... Stage 3: Complete!

===================================================================================================
                                         ~ Break ~
===================================================================================================

Stage 3 was quite long, you can now relax and have a break, I certainly will myself :)



===================================================================================================
                      =#= Stage 4: Four TokyoNauts manage Character Encoding =#=
===================================================================================================

Let's go back to work. I personnaly took a few weeks break ;) What about you?
Everything we are dealing with until this point is a bunch of bytes, you can say plain old binary data.
We now want to make sense and distinguish the characters out of the data. Back in ye ol' days, the
problem was a no-brainer: 1 byte = 1 character, that's ASCII. But now in 2008, computer makers, as
you as a software programmer, are more aware of internationalization issues, and have fully understood
that thousands of chinese or japanese characters definitely cannot fit in the 256 values of one byte.

So here comes Unicode, a worlwide standard that defines a numeric value (or code point) to each
and every known character in current and past languages of the world (with more in mind). Of course
Unicode values do not fit into a single byte. So we will use here an int value, worth 4 bytes of data,
which is enough for all Unicode values with a comfortable margin (at least for now, one byte in ASCII 
used to look comfortable as well at one time...).

Now as a matter of fact, this Unicode value is not written as if in files at the byte level, because
software engineer are clever people who took into account backward-compatibility with ASCII, and tried
to optimize the size of files based on the fact that the most frequent characters should have the smaller
representation. And since your mileage may vary based on the language you are speaking and writing the
most, different encodings have been defined for character values, with some being more popular in Western
countries as US-ASCII or ISO-8859-1, while Shift_JIS or BIG5 would be more popular in Japan and China
because they can actually represent Japanese and Chinese while US-ASCII and ISO-8859-1 cannot. If you have 
a look at your web browser, there is a menu that allows you to select the encoding of the current page.
If you change this value from the auto-detect mode, the text will change, and may end up looking like
garbage if you choose an encoding very foreign with respect to the text of the web page. My browser
groups some of these encodings in a "Unicode" category, which includes:
 * Unicode (UTF-8)
 * Unicode (UTF-16 Little Endian)
 * Unicode (UTF-16 Big Endian)
 * Unicode (UTF-32 Little Endian)
 * Unicode (UTF-32 Big Endian)
 * Unicode (UTF-7)

These encodings do not define different versions of Unicode, they have in common that they can store
most if not all unicode values, using variable length encoding algorithms, that you can say are quite 
clever compared to the fixed length encoding of ASCII on a single byte. The difference between these 
different algorithms, or encodings, is the minimum number of bits required for a single character.
In ASCII both the minimum and maximum size is 1 byte or 8 bits, so this would be ASCII-8 in a way :)
For ASCII characters, which have a low codepoint, UTF-8 will use 8 bits, UTF-16 16 bits, UTF-32 
32 bits while UTF-7 would use only 7. The distinction between Little Endian and Big Endian is about 
a simple matter of taste: which byte comes first, is it the most significant as in the way we write 
decimal numbers, where the 3 on the left of 3745 weights much more than the 5 on the right, or the 
other way round. This is made configurable in UTF-* files with the use of the special BOM character 
or Byte-Order-Mark, used as the first byte sequence, which is read differently in LE and BE, and so 
you can decide which encoding was applied.

You may now know a little more about character encoding, and why it actually matters (well, it 
certainly does).

Since Java evolved in this world in transformation, it took into account from the beginning this
difference between byte and character, and used a variant of UTF-16 encoding to store unicode values
as characters. As time shows, the move was not bold enough however, since Java only allocated 2 bytes 
to a char value, which used to be enough for Unicode values defined at that time, but no longer is 
as some require more than 2 bytes or 16 bits (version 3.2 of Unicode already defined "30,000 more 
characters than can be squeezed into two bytes" [1]). These characters have nothing special except that 
they were late to the party, but they have now a special status in Java by design, since they cannot 
be represented in a single char value. So they are called "surrogate" characters, and they are represented 
with a pair of char values, which is, exactly, worth 4 bytes or an int value.

[1] 10 Reasons We Need Java 3.0
2002-07-31, by Elliotte Rusty Harold
http://www.onjava.com/pub/a/onjava/2002/07/31/java3.html

If we take the example of the "surrogate character" 2F81A mentionned in the documentation of the Java
Character class, it will be represented in Java as a pair of 16-bit char values, while in the Tokyo
Project it is really a single character Token, with value 0x2F81A.

These so-called "surrogate pairs" require some specific handling in programs, some special tests in 
every code managing characters, like a text editor, or they will get it wrong and consider each part
as a single character (which they aren't). And sadly, most text editors in Java do get it wrong, and
display surrogate pairs as two characters, or like the one I am currently using, a character that takes
two moves of the right arrow to get the cursor moving...

All this to tell you why I decided not to rely on Java characters, but use integer values instead, 
corresponding to the Unicode code point. This is the reason why if you have a look at the ITokyoNaut
interface documentation, and you check the common constant values defined for LANGUAGE, you will find
LANGUAGE_BINARY, LANGUAGE_UNICODE_TEXT and LANGUAGE_ERROR, no mention of these Java characters. This
way the basic way to manipulate character tokens in the Tokyo Project is to manipulate tokens in the
language LANGUAGE_UNICODE_TEXT, which does not include a complete list of allowed values, because
it's been already done by Unicode: each token code in this language is an integer Unicode code point.

But Java of course features a whole lot of utility classes that help you perform the conversion
from binary to characters and the contrary. For this prototype, I would like to keep the best of
both worlds, i.e. reuse this code made available to me (I don't want to rewrite one of the converters,
because I once wrote that kind of code, and it is a real pain) while throwing away the flawness of
Java characters.

In order to do that, I will chain two new TokyoNauts, the first one performing the conversion from
binary to Java characters, let's call it BinaryToJavaCharNaut based on chosen character encoding,
while the second, let's call it JavaCharToUnicodeNaut will output the useful character tokens with
plain Unicode code points.

As you notice, the combination of BinaryToJavaCharNaut and JavaCharToUnicodeNaut is equivalent to
a single BinaryToUnicodeNaut (and we could do it that way, but it's just simpler in two steps).

Remember these "surrogate pairs" we mentionned before? One big responsibility of the 
JavaCharToUnicodeNaut is to combine them into a single Unicode token, while for other characters,
it simply translates the char value to Unicode int code point.

The processing in the two TokyoNauts is illustrated below:

  // In BinaryToJavaCharNaut:
  public boolean areWeThereYet(int[] meta, byte[] data)
  {
    if ( super.areWeThereYet(meta,data) || _decoder==null || _outChars==null )
      return true;
    
    do
    {
      if (  _isUnderflow  &&  ( !_outChars.hasRemaining() )  )
      {
        if ( _inBytes!=null && _inBytes.hasRemaining() )
        {
          meta[LANGUAGE]=LANGUAGE_BINARY;
          meta[TOKEN]=TOKEN_REMAIN;
          meta[OFFSET]=0;
          meta[LENGTH]=_inBytes.remaining();
          _inBytes.compact();
        }
        
        if ( _src.areWeThereYet(meta,data) )
        {
          // Finalize
          if (_inBytes!=null)
          {
            CoderResult lastResult = _decoder.decode(_inBytes,_outChars,true);
            if ( lastResult.isError() )
            {
              meta[LANGUAGE]=LANGUAGE_ERROR;
              meta[TOKEN]=0x30F;
            }
          }
          return true;
        }
        // Skip unknown tokens
        if ( meta[LANGUAGE]!=LANGUAGE_BINARY || meta[TOKEN]!=TOKEN_BINARY )
          return false;
      
        // Load input data
        if ( _inBytes==null || _inBytes.array()!=data )
        {
          _inBytes = ByteBuffer.wrap(data,meta[OFFSET],meta[LENGTH]);
        }
        else
        {
          _inBytes.position(meta[OFFSET]);
          _inBytes.limit(meta[OFFSET]+meta[LENGTH]);
        }
        
        _isUnderflow = false;
      }
      
      if ( !_outChars.hasRemaining() )
      {
        _charStartOffset = _inBytes.position();
        _outChars.clear(); // set full buffer as storage space, before writing
        CoderResult decodingResult = _decoder.decode(_inBytes,_outChars,false);
        _outChars.flip(); // flip is required before reading or checking remaining()
        
        if ( decodingResult.isError() )
        {
          meta[LANGUAGE]=LANGUAGE_ERROR;
          meta[TOKEN]=(decodingResult.isMalformed()? 0x311 : 0x312);
          meta[OFFSET]=_inBytes.position();
          meta[LENGTH]=decodingResult.length();
          _inBytes.position( _inBytes.position()+decodingResult.length() );
          return false;
        }
        
        if ( decodingResult.isUnderflow() )
          _isUnderflow = true;
      }
      
      if ( _outChars.hasRemaining() )
      {
        meta[LANGUAGE] = LANGUAGE_JAVA_CHAR;
        meta[TOKEN] = _outChars.get();;
        meta[LEFT] = LEFT_START;
        meta[RIGHT] = RIGHT_END;
        meta[OFFSET] = _charStartOffset;
        meta[LENGTH] = _inBytes.position()-_charStartOffset;
        return false;
      }
    }
    while(true);
  }
  

  // In JavaCharToUnicodeNaut.java
  (PASTE CODE HERE)

You can notice the introduction of a new token, TOKEN_REMAIN, used to report some data part of an
incomplete token to the source, to be patched with more data with next delivery. The updated code
including management of TOKEN_REMAIN in FileToBinaryNaut now looks like:

  // In FileToBinaryNaut.java:
  public boolean areWeThereYet(int[] meta, byte[] data)
  {
    if ( super.areWeThereYet(meta,data) || _in==null )
      return true;
    
    int writeOffset;
    int writeLength;
    
    if (meta[TOKEN]==TOKEN_REMAIN)
    {
      writeOffset = meta[LENGTH];
      writeLength = data.length-writeOffset;
    }
    else
    {
      writeOffset = 0;
      writeLength = data.length;
    }
    
    try
    {
      if ( _in.available()==0 )
        return true;
      
      int bytesRead = _in.read(data,writeOffset,writeLength);
      
      meta[OFFSET] = 0;
      meta[LENGTH] = (bytesRead==-1? writeOffset : writeOffset+bytesRead);
      
      meta[LEFT] = (meta[TOKEN]==TOKEN_SPARK? LEFT_START: LEFT_CONTINUED);
      meta[RIGHT] = (_in.available()==0? RIGHT_END : RIGHT_CONTINUED);
      
      meta[LANGUAGE] = LANGUAGE_BINARY;
      meta[TOKEN] = TOKEN_BINARY;
    }
    catch(Exception e)
    {
      System.err.println("Error in FileToBinaryNaut#areWeThereYet(): "+e);
      meta[LANGUAGE]=LANGUAGE_ERROR;
      meta[TOKEN]=0x101;
      return true;
    }
    
    return false;
  }



===================================================================================================
                                        ~ The End ~
===================================================================================================

* FUTURE DIRECTION: AUTOMATION USING GRAMMAR DEFINITIONS
  Inspired by the example of compiler program generation based on grammar definition,
  I am quite convinced that the same could prove useful, given appropriate required 
  adaptations, to transform data, e.g. to get an XML view of binary data:
  
  - define the grammar of your data
  - define the vocabulary for your data viewed as XML
  - define the syntax of your queries, e.g. the required XPath subset
  - use them to create 
      - one tool reading your data and implementing an XML parsing API,
        with the following possible components:
        * Query parsing and analysis
        * State Machine for Data Grammar
        * State Machine/Factory for Data Extraction
        
      - one tool providing an XML serializing API and writing your data,
        with the following possible components:
        * Query parsing and analysis
        * State Machine for Data Grammar
        * State Machine/Factory for Data Serialization
      
      - and maybe additional tools to validate input data / generated data
      
  - plug your tool on applications processing XML

  NOTE:
    - you may need some adapters for the expected API or extra conversion steps
    - the application has to know the vocabulary of your data viewed as XML 
      to understand its semantic
  
  At the core lies the need to formalize the grammar of your data, which could be done with 
  several to be determined notations (EBNF, tables, encoding algorithm,...). I mean here that 
  the tool should be extensible to support several grammars, not be limitated to a single one.
  
  Adapted to handle binary data and not just text and character tokens, the grammar should define:
    - some structure, which will be mapped to the XML tree structure
    - some codecs, defining the format of leaf nodes
    - some comments, explaining the semantics of different fields
    - there may also be some additional relations between different elements,
    that must be understood for proper decoding (choice, repeat, based on values read before)
  
  This project may end up defining a general format good enough for 80% of grammar definitions,
  or using different grammar types for different formats.
        
=============== END OF DOCUMENT: Tokyo Project - Process ===============