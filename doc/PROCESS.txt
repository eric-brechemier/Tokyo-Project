==================================================================================
 The Tokyo Project is hosted on GitHub:
 https://github.com/eric-brechemier/Tokyo-Project
 
 Copyright (c) 2005-2008 Eric Bréchemier
 http://eric.brechemier.name
 Licensed under BSD License and/or MIT License.
 See: http://creativecommons.org/licenses/BSD/
==================================================================================

==========================================
Title: Tokyo Project - Process Definition
Last modified: 2008-02-17
By: Eric Bréchemier
Encoding: UTF-8
==========================================

                        *******************************   
                        **       Tokyo Project       **
                        **     Process Definition    **
                        *******************************


* OBJECTIVES
- provide an XML view of non-XML data
- let applications view and create non-XML documents as easily as XML ones
- transformation of non-XML documents to XML and back again

* VISION
- we start with:
    - on the one hand, lots of existing applications working on xml data,
      lots of developers with experience in xml processing,
    - on the other hand, many remaining non-xml formats
      (e.g. for pure binary or pure text data)

- handling non-XML data as XML requires two transformations:
    - from non-xml to xml, for applications input,
    corresponding to XML parsing
    - from xml to non-xml, for applications output,
    corresponding to XML serialization

- an XML parser works on standard XML files/data and implements some XML Parser API (DOM, SAX,...)
  Applications do not access XML data directly, but through the provided API
  
- an XML serializer implements some XML Serializer API (DOM,XML Pull,StAX...).
  Applications often do not create XML data directly, but through the provided API.
  Some applications create XML data directly at the text level, with a high risk of producing non-xml
  i.e. not well-formed xml.
    
- the structure of an XML document is explicit, it uses the same syntax for all XML documents,
  no matter which grammar/vocabulary they use. Parsers can process this data without any knowledge
  of the document semantics.
  Structure in non-XML data is often implicit or uses a very specific syntax.
  Applications often need to know some grammar to be able to parse the data.

* A SAMPLE SIMPLE PROCESS
Enough concepts! Let me introduce a simple sample process, or a sample simple process if you prefer,
applied to the first prototype: How to sort CSV records using XSLT.

We will grow a working prototype step by step, using an iterative process. We will start with a basic
chain of two TokyoNauts performing a file copy by transferring buffers from a Source to a Destination,
then insert more TokyoNauts in pairs to handle UTF-8 Text encoding, CSV, SAX, and a final one playing 
the role of a Bridge for XSLT Transformation.

You can refer to the files of this example in the folder "prototype-1-csv2xml" of the Tokyo Project 
download archive, or in the trunk of the source code on Subversion repository.

===================================================================================================
                      =#= Stage 1: Skeleton for the main loop =#=
===================================================================================================
                      
The first step will be to write the main loop, which we call ProtoOneMainLoop because it is the 
Prototype One, and it is the main loop. This class has a single method main(), the entry point 
for the complete application.

  package net.sf.tokyo.prototype1;
  
  public class ProtoOneMainLoop
  {
    public static void main(String[] args)
    {
      
    } 
  }

We can run this prototype, and it does nothing. Stage 1: Clear.

===================================================================================================
                      =#= Stage 2: First main loop, Source to Destination =#=
===================================================================================================

For the first version of the main loop, we will start with two TokyoNauts:
  - a Source reading bytes from a file
  - a Destination writing bytes to a file

We'll need:
  - code to set up the chain of two TokyoNauts,
  - main loop code to run the processing
  - one class for the first TokyoNaut: FileToBinaryNaut (reading file into buffers of binary data)
  - one class for the last TokyoNaut: BinaryToFileNaut (writing file from buffers of binary data)

Let's go! As a starter, let's copy and paste the code sample for the main loop featured in the 
ITokyoNaut#areWeThereYet() javadoc:

  public class ProtoOneMainLoop
  {
    public static void main(String[] args)
    {
      int[] meta = new int[7];
      
      int step = 0;
      final int STEP_LIMIT = 99;
      
      while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
      {
        if ( meta[ITokyoNaut.VERSION] == ITokyoNaut.VERSION_NANA)
        {
          System.out.println
            (  
               "Step: "+step+"\n\t"
              +"Language: 0x"+Integer.toHexString(meta[ITokyoNaut.LANGUAGE])+"\n\t"
              +"Token: 0x"+Integer.toHexString(meta[ITokyoNaut.TOKEN])+"\n\t"
              +"Left Relation: "+(meta[ITokyoNaut.LEFT]==1?"+":"")+meta[ITokyoNaut.LEFT]+"\n\t"
              +"Fragment Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
              +"Fragment Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
              +"Right Relation: "+meta[ITokyoNaut.RIGHT]+"\n"
            );
        }
      }
    } 
  }
  
I now get 3 errors at compiling because chain and destination are not defined:
  [javac] Compiling 1 source file to /home/enz/projects/tokyo/trunk/prototype-1-csv2xml/out/class
  [javac] /home/enz/projects/tokyo/trunk/prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/ProtoOneMainLoop.java:61: cannot find symbol
  [javac] symbol  : variable chain
  [javac] location: class net.sf.tokyo.prototype1.ProtoOneMainLoop
  [javac]     while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
  [javac]                                                                   ^
  [javac] /home/enz/projects/tokyo/trunk/prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/ProtoOneMainLoop.java:61: cannot find symbol
  [javac] symbol  : variable chain
  [javac] location: class net.sf.tokyo.prototype1.ProtoOneMainLoop
  [javac]     while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
  [javac]                                                                         ^
  [javac] /home/enz/projects/tokyo/trunk/prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/ProtoOneMainLoop.java:61: cannot find symbol
  [javac] symbol  : variable destination
  [javac] location: class net.sf.tokyo.prototype1.ProtoOneMainLoop
  [javac]     while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
  [javac]                                         ^
  [javac] 3 errors
  
This is because we still have no TokyoNaut and chain and destination have not been initialized.
To pass this step, we will create two TokyoNaut classes with a null implementation, import them here,
initialize them and recompile...

It happens that I already have a Null TokyoNaut implementation at hand, part of the "test" section
of the main package: main/test/java/net/sf/tokyo/test/NullTokyoNaut.java

It is not part of the Tokyo API library, but I can copy and paste this file as a template for my two 
new TokyoNauts. So I copy main/test/java/net/sf/tokyo/test/NullTokyoNaut.java
to prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/tokyonauts/FileToBinaryNaut.java 
and prototype-1-csv2xml/src/java/net/sf/tokyo/prototype1/tokyonauts/BinaryToFileNaut.java
then change the class name and package name to match the file and location.

Here is what the FileToBinaryNaut looks like:

  package net.sf.tokyo.prototype1.tokyonauts;
  
  import net.sf.tokyo.ITokyoNaut;
  
  /**
   * Null implementation of ITokyoNaut interface.<br/>
   *
   * <p>
   * Does nothing and never terminates which means that if used without boundary, 
   * it will run in a main loop forever.
   * This source code can be used as a starting point for TokyoNaut implementations.
   * </p>
   */
  public class FileToBinaryNaut implements ITokyoNaut
  {
    protected byte[] _nullData = new byte[0];
    
    public boolean areWeThereYet(ITokyoNaut[] chain, int position, int[] meta)
    {
      meta[VERSION] = VERSION_NANA;
      meta[LANGUAGE] = LANGUAGE_BINARY;
      meta[TOKEN] = TOKEN_BINARY;
      meta[LEFT] = LEFT_START;
      meta[OFFSET] = 0;
      meta[LENGTH] = 0;
      meta[RIGHT] = RIGHT_END;
      
      int destination = position+1;
      if (destination<chain.length)
        chain[destination].notYet(this,_nullData);
      return false;
    }
    
    public void notYet(ITokyoNaut source, byte[] data)
    {
    }
    
  }

I now import the two TokyoNauts in ProtoOneMainLoop, and add the code below at the start of the main()
method to initialize the two instances, source and destination, and plug them together in the chain array:
  
  public static void main(String[] args)
  {
    FileToBinaryNaut source = new FileToBinaryNaut();
    BinaryToFileNaut destination = new BinaryToFileNaut();
    ITokyoNaut[] chain = {source, destination};
    
    (...) // following code unchanged
  }

It not only compiles again, but also the loop runs correctly for 99 iterations then stops. 
You can say, Stage 2: Complete.

===================================================================================================
                      =#= Stage 3: Two TokyoNauts perform File to File Copy =#=
===================================================================================================

Now it's time to actually perform something useful with our two TokyoNauts. To read one file and
write another, we need the actual locations of the two files. We will add one parameter with the file
location to the constructor of each TokyoNaut, and get these two values from the command line.
To keep a little more freedom, we will actually request only the ouput folder location, and append the 
file name, which will let us create several different files in this folder if needed, e.g. to compare
the output of different tests:

  public static void main(String[] args)
  {
    if (args.length <2)
    {
      System.out.println("Usage: [ProtoOneMain] inFilePath outDirPath");
      return;
    }
    String inFilePath = args[0];
    String outDirPath = args[1];
    String outFilePath = outDirPath + "/result.csv";
    
    System.out.println("Starting ProtoOne:"
      + "\n  In: " + inFilePath
      + "\n  Out: " + outFilePath
    );
    
    FileToBinaryNaut source = new FileToBinaryNaut(inFilePath);
    BinaryToFileNaut destination = new BinaryToFileNaut(outFilePath);
    ITokyoNaut[] chain = {source, destination};
    
    int[] meta = new int[7];
    
    int step = 0;
    final int STEP_LIMIT = 99;
    
    while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
    {
      if ( meta[ITokyoNaut.VERSION] == ITokyoNaut.VERSION_NANA)
      {
        System.out.println
          (  
             "Step: "+step+"\n\t"
            +"Language: 0x"+Integer.toHexString(meta[ITokyoNaut.LANGUAGE])+"\n\t"
            +"Token: 0x"+Integer.toHexString(meta[ITokyoNaut.TOKEN])+"\n\t"
            +"Left Relation: "+(meta[ITokyoNaut.LEFT]==1?"+":"")+meta[ITokyoNaut.LEFT]+"\n\t"
            +"Fragment Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
            +"Fragment Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
            +"Right Relation: "+meta[ITokyoNaut.RIGHT]+"\n"
          );
      }
    }
    
    System.out.println("Tokyo ProtoOne completed.");
  }

After adding the code in FileToBinaryNaut and BinaryToFileNaut with the new constructor (you can leave
it empty for now) everything runs fine with no change.

We now add the code to initialize a FileInputStream in FileToBinaryNaut constructor, and a 
FileOutputStream in BinaryToFileNaut:

  // in FileToBinaryNaut.java,
  protected FileInputStream _in;
  
  public FileToBinaryNaut(String filePath)
  {
    try 
    {
      _in = new FileInputStream(filePath);
    }
    catch(Exception e)
    {
      System.err.println("Error opening file for input in FileToBinaryNaut(): "+e);
    }
  }
  
  // in BinaryToFileNaut.java,
  protected FileOutputStream _out;
  
  public BinaryToFileNaut(String filePath)
  {
    try 
    {
      _out = new FileOutputStream(filePath);
    }
    catch(Exception e)
    {
      System.err.println("Error opening file for output in BinaryToFileNaut(): "+e);
    }
  }

Don't forget to add the imports,
  // in FileToBinaryNaut.java,
  import java.io.FileInputStream;
  
  // in BinaryToFileNaut.java,
  import java.io.FileOutputStream;

Finally, we've just completed the plumbing, and we arrive to the core of each TokyoNaut processing,
within areWeThereYet() method. Here is the resulting code for FileToBinaryNaut:
  
  // in FileToBinaryNaut.java,
  package net.sf.tokyo.prototype1.tokyonauts;
  
  import java.io.FileInputStream;
  
  import net.sf.tokyo.ITokyoNaut;
  
  /**
   * TokyoNaut reading binary tokens from a file.<br/>
   *
   * <p>
   *   <ul>
   *     <li><em>Role:</em> Source</li>
   *     <li><em>Input Consumed:</em> None</li>
   *     <li><em>Output Produced:</em>
   *       {@link net.sf.tokyo.ITokyoNaut#TOKEN_BINARY TOKEN_BINARY} tokens 
   *       in {@link net.sf.tokyo.ITokyoNaut#LANGUAGE_BINARY LANGUAGE_BINARY}
   *     </li>
   *     <li><em>Errors Produced: (* fatal errors)</em>
   *       <ul>
   *         <li><code>0x101*</code> - I/O error reading or checking bytes available in file</li>
   *       </ul>
   *     </li>
   *   </ul>
   * </p>
   */
  public class FileToBinaryNaut implements ITokyoNaut
  {
    protected FileInputStream _in;
    protected byte[] _data;
    protected boolean _isStart;
    
    public FileToBinaryNaut(String filePath)
    {
      try 
      {
        _in = new FileInputStream(filePath);
      }
      catch(Exception e)
      {
        System.err.println("Error opening file for input in FileToBinaryNaut(): "+e);
      }
      
      _isStart = true;
      _data = new byte[10];
    }
    
    public boolean areWeThereYet(ITokyoNaut[] chain, int position, int[] meta)
    {
      meta[VERSION] = VERSION_NANA;
      meta[LANGUAGE] = LANGUAGE_BINARY;
      meta[TOKEN] = TOKEN_BINARY;
      meta[LEFT] = (_isStart? LEFT_START: LEFT_CONTINUED);
      _isStart = false;
      meta[OFFSET] = 0;
      meta[LENGTH] = _data.length;
      
      try
      {
        if ( _in.available()==0 )
        {
          _terminate();
          return true;
        }
        
        int bytesRead = _in.read(_data,meta[OFFSET],meta[LENGTH]);
        meta[LENGTH] = (bytesRead==-1? 0 : bytesRead);
        meta[RIGHT] = (_in.available()==0? RIGHT_END : RIGHT_CONTINUED);
      }
      catch(Exception e)
      {
        System.err.println("Error in FileToBinaryNaut#areWeThereYet(): "+e);
        meta[LANGUAGE]=LANGUAGE_ERROR;
        meta[TOKEN]=0x101;
        _terminate();
        return true;
      }
      
      int destination = position+1;
      if (destination<chain.length)
        chain[destination].notYet(this,_data);
      return false;
    }
    
    protected void _terminate()
    {
      try
      {
        if (_in!=null)
            _in.close();
        _in = null;
      }
      catch(Exception e)
      {
        System.err.println("Error closing file in FileToBinaryNaut#_terminate(): "+e);
      }
    }
    
    public void notYet(ITokyoNaut source, byte[] data)
    {
    }
    
  }
  
You can notice that this code creates TOKEN_BINARY events, as many as required, one at a time.
Thanks to the protected member _isStart, which I added to the class, and initialized to true in 
the constructor, the first token has meta[LEFT] = LEFT_START and following tokens have 
meta[LEFT] = LEFT_CONTINUED. The last token is created when no more data is available for next run;
it has meta[RIGHT] = RIGHT_END, while preceding tokens have meta[RIGHT] = RIGHT_CONTINUED.
This mechanism allows consuming TokyoNauts to detect the start and end of each Token produced.

I also introduced a new field _data, initialized in the constructor with an array of 10 bytes; this is 
an arbitrary length chosen quite low to make behaviors at the boundary more explicit in the context
of this prototype. I also removed the field _nullData and created a _terminate method used to properly
close the input file at the end of processing.

In case of error while reading input, meta is filled with an error token using LANGUAGE_ERROR
and an arbitrary error code, 0x101, unique in the context of my own prototype application.

The code for BinaryToFileNaut is quite similar; the main difference lies in the fact that BinaryToFileNaut 
relies on "source" TokyoNaut to provide input data using the notYet() callback, and terminates when no 
more data is available:
  
  // in BinaryToFileNaut.java,
  package net.sf.tokyo.prototype1.tokyonauts;
  
  import java.io.FileOutputStream;
  
  import net.sf.tokyo.ITokyoNaut;
  
  /**
   * TokyoNaut writing tokens to a file.<br/>
   *
   * <p>
   *   <ul>
   *     <li><em>Role:</em> Destination</li>
   *     <li><em>Input Consumed:</em> Any non-error token</li>
   *     <li><em>Output Produced:</em> None</li>
   *     <li><em>Side effect:</em> Writes all incoming data fragments to a file</li>
   *     <li><em>Errors Produced: (* fatal errors)</em>
   *       <ul>
   *         <li><code>0x200*</code> - no preceding TokyoNaut available as Source</li>
   *         <li><code>0x201</code> - data expected from source TokyoNaut was not received</li>
   *         <li><code>0x202</code> - I/O error writing bytes in file</li>
   *       </ul>
   *     </li>
   *   </ul>
   * </p>
   */
  public class BinaryToFileNaut implements ITokyoNaut
  {
    protected byte[] _data;
    protected FileOutputStream _out;
    
    public BinaryToFileNaut(String filePath)
    {
      try
      {
        _out = new FileOutputStream(filePath);
      }
      catch(Exception e)
      {
        System.err.println("Error opening file for output in BinaryToFileNaut(): "+e);
      }
    }
    
    public boolean areWeThereYet(ITokyoNaut[] chain, int position, int[] meta)
    {
      int source = position-1;
      if (source<0)
      {
        meta[LANGUAGE]=LANGUAGE_ERROR;
        meta[TOKEN]=0x200;
        _terminate();
        return true;
      }
      
      if ( chain[source].areWeThereYet(chain,source,meta) )
      {
        _terminate();
        return true;
      }
      
      if (_data==null)
      {
        meta[LANGUAGE]=LANGUAGE_ERROR;
        meta[TOKEN]=0x201;
        return false;
      }
      
      try
      {
        _out.write(_data,meta[OFFSET],meta[LENGTH]);
      }
      catch(Exception e)
      {
        System.err.println("Error in BinaryToFileNaut#areWeThereYet(): "+e);
        meta[LANGUAGE]=LANGUAGE_ERROR;
        meta[TOKEN]=0x201;
        return false;
      }
      
      int destination = position+1;
      if (destination<chain.length)
        chain[destination].notYet(this,_data);
      return false;
    }
    
    public void notYet(ITokyoNaut source, byte[] data)
    {
      _data = data;
    }
    
    protected void _terminate()
    {
      try
      {
        if (_out!=null)
          _out.close();
        _out = null;
      }
      catch(Exception e)
      {
        System.err.println("Error closing file in BinaryToFileNaut#_terminate() "+e);
      }
    }
    
  }

Now if you followed me till this point, you can run this code and check the file "result.csv" created in 
"out/data"; it is a carbon copy of the input file... Stage 3: Complete!

===================================================================================================
                                         ~ Break ~
===================================================================================================

Stage 3 was quite long, you can now relax and have a break, I certainly will myself :)



===================================================================================================
                      =#= Stage 4: Four TokyoNauts manage Character Encoding =#=
===================================================================================================

Let's go back to work. I personnaly took a few weeks break ;) What about you?
Everything we are dealing with until this point is a bunch of bytes, you can say plain old binary data.
We now want to make sense of it and extract the characters out of the binary data. Back in ye ol' days, 
the problem was a no-brainer: in ASCII, all character values fit in 7 bits within a single byte, so you 
could easily assume that 1 byte = 1 character. But now in 2008, computer makers, as you as a software 
programmer, are more aware of internationalization issues, and have fully understood that thousands of 
chinese or japanese characters definitely cannot fit in the 256 values of one byte.

So here comes Unicode, a worlwide standard that defines a numeric value (or code point) to each
and every known character in current and past languages of the world (with more in mind). Of course
Unicode values do not fit into a single byte. So we will use here an int value, worth 4 bytes of data,
which is enough for all Unicode values with a comfortable margin (at least in a foreseeable future, but
who knows... one byte in ASCII used to look comfortable as well at one time...).

Now as a matter of fact, this Unicode value is usually not written as if in files at the byte level, 
because software engineers are clever people who took into account backward-compatibility with ASCII, 
and tried to optimize the size of files based on the fact that the most frequent characters should have 
the smaller representation. And since your mileage may vary based on the language you are speaking and 
writing the most, different encodings have been defined for character values, with some being more popular 
in Western countries as US-ASCII or ISO-8859-1, while Shift_JIS or BIG5 would be more popular in Japan and 
China because they can actually represent Japanese and Chinese while US-ASCII and ISO-8859-1 cannot. 
If you have a look at your web browser, there is a menu that allows you to select the encoding of the 
current page. If you change this value from the auto-detect mode, the text will change, and may end up 
looking like garbage if you choose an encoding very foreign with respect to the text of the web page. 
My browser groups some of these encodings in a "Unicode" category, which includes:
 * Unicode (UTF-8)
 * Unicode (UTF-16 Little Endian)
 * Unicode (UTF-16 Big Endian)
 * Unicode (UTF-32 Little Endian)
 * Unicode (UTF-32 Big Endian)
 * Unicode (UTF-7)

These encodings do not define different versions of Unicode, they just offer different representations 
for the same unicode integer values. They have in common that they can represent most if not all unicode 
values, using different variable length encoding algorithms, that you can say are quite clever compared 
to the fixed length encoding of ASCII on a single byte. The difference between these different algorithms, 
or encodings, is the minimum number of bits required for a single character. In ASCII both the minimum and 
maximum size is 7 bits. On the other hand, for ASCII characters, which have a low codepoint, UTF-8 will 
use 8 bits, UTF-16 16 bits, UTF-32 32 bits while UTF-7 would use only 7. The distinction between Little 
Endian and Big Endian is about a simple matter of taste: which byte comes first, is it the most significant 
as in the way we write decimal numbers, where the 3 on the left of 3745 weights much more than the 5 on the 
right, or the other way round. The so-called endianness can be detected in UTF-* files with the use of the 
special BOM character or Byte-Order-Mark, used as the first byte sequence, which is read differently in LE 
and BE, and so you can decide which encoding was applied.

You may now know a little more about character encoding, and why it actually matters (well, it certainly 
does).

Since Java evolved in this world in transformation, its specification took into account from the beginning 
this difference between byte and character, and Java uses a variant of UTF-16 encoding to store unicode 
values as characters. As time shows, the move was not bold enough however, since Java only allocated 
2 bytes to a char value, which used to be enough for Unicode values defined at that time, but no longer is 
as some require more than 2 bytes or 16 bits (version 3.2 of Unicode already defined "30,000 more 
characters than can be squeezed into two bytes" [1]). These characters have nothing special except that 
they were late to the party, but they have now a special status in Java by design, since they cannot 
be represented in a single char value. So they are called "surrogate" characters, and they are represented 
with a pair of char values, which is, exactly, worth 4 bytes or an int value. The concept of "surrogate
characters" actually comes from UTF-16, being as we have seen the model for Java design in that space.

[1] 10 Reasons We Need Java 3.0
2002-07-31, by Elliotte Rusty Harold
http://www.onjava.com/pub/a/onjava/2002/07/31/java3.html

If we take the example of the "surrogate character" 2F81A mentionned in the documentation of the Java
Character class, it will be represented in Java as a pair of 16-bit char values, while in the Tokyo
Project it is really a single character Token, with value 0x2F81A.

These so-called "surrogate pairs" require some specific handling in programs, some special tests in 
every code managing characters, like a text editor, or they will get it wrong and consider each part
as a single character (which they aren't). And sadly, most text editors in Java do get it wrong, and
display surrogate pairs as two characters, or like the one I am currently using, a character that takes
two moves of the right arrow to get the cursor moving...

All this to tell you why I decided not to rely on Java characters, but to use instead integer values 
corresponding to the Unicode code point. This is the reason why if you have a look at the ITokyoNaut
interface documentation, and you check the common constant values defined for LANGUAGE, you will find
LANGUAGE_BINARY, LANGUAGE_UNICODE_TEXT and LANGUAGE_ERROR, no mention of these Java characters. This
way the basic way to manipulate character tokens in the Tokyo Project is to manipulate tokens in the
language LANGUAGE_UNICODE_TEXT, which does not include a complete list of allowed values, because
it's been already done by Unicode: each token code in this language is an integer Unicode code point.

I will now introduce two new TokyoNauts, UTF8ToUnicodeNaut and UnicodeToUTF8Naut, handling UTF-8 parsing
to Unicode integer and UTF-8 serialization from Unicode integer.

But first, let's refine a little bit more the main loop, to report in more details what happens
inside and at the end of the main loop:

  // in ProtoOneMainLoop.java
  
  (...)
  
  // main loop code
  int step = 0;
    final int STEP_LIMIT = 99;
    
    while(  (step++ < STEP_LIMIT)  &&  !destination.areWeThereYet(chain,chain.length-1,meta)  )
    {
      if ( meta[ITokyoNaut.VERSION] == ITokyoNaut.VERSION_NANA)
      {
        System.out.println
          (  
             "Step: "+step+"\n\t"
            +"Language: 0x"+Integer.toHexString(meta[ITokyoNaut.LANGUAGE])+"\n\t"
            +"Token: 0x"+Integer.toHexString(meta[ITokyoNaut.TOKEN])+"\n\t"
            +"Left Relation: "+(meta[ITokyoNaut.LEFT]==1?"+":"")+meta[ITokyoNaut.LEFT]+"\n\t"
            +"Fragment Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
            +"Fragment Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
            +"Right Relation: "+meta[ITokyoNaut.RIGHT]+"\n"
          );
      }
      else
      {
        System.out.println
          (  
             "Step: "+step+"\n\t"
            +"Language: 0x"+Integer.toHexString(meta[ITokyoNaut.LANGUAGE])+"\n\t"
          );
      }
    }
    
    System.out.println("Tokyo ProtoOne completed at step: "+(step-1) );
    
    if ( (step-1)==STEP_LIMIT)
      System.out.println("Step limit reached: "+STEP_LIMIT);
    
    if (meta[ITokyoNaut.LANGUAGE] == ITokyoNaut.LANGUAGE_ERROR)
    {
      System.out.println
          (  
             "Final Error: \n\t"
            +"Code: 0x"+Integer.toHexString(meta[ITokyoNaut.TOKEN])+"\n\t"
            +"at Offset: "+meta[ITokyoNaut.OFFSET]+"\n\t"
            +"and Length: "+meta[ITokyoNaut.LENGTH]+"\n\t"
          );
    }
  

Both UTF8ToUnicodeNaut and UnicodeToUTF8Naut can be derived from a common filter structure, illustrated 
below:
  public class UTF8ToUnicodeNaut implements ITokyoNaut
  {
    protected int[]  _meta;
    protected byte[] _data;
    
    protected int _offsetRead;
    protected int _lengthRead;
    
    public UTF8ToUnicodeNaut()
    {
      _init();
    }
    
    protected void _init()
    {
      _meta = new int[7];
      _data = null;
      
      _offsetRead = -1;
      _lengthRead = 0;
    }
    
    protected void _terminate()
    {
      _meta = null;
      _data = null;
    }
    
    protected boolean _isError(int[] meta)
    {
      return meta[LANGUAGE]==LANGUAGE_ERROR;
    }
    
    protected boolean _isInputEmpty()
    {
      return 
      (
            _data == null
        ||  _meta[OFFSET]+_meta[LENGTH] == _offsetRead+_lengthRead
      );
    }
    
    protected void _loadInputData(ITokyoNaut[] chain, int position, int[] meta)
    {
      _data = null;
      _offsetRead = 0;
      _lengthRead = -1;
      
      if (position<0)
      {
        meta[LANGUAGE]=LANGUAGE_ERROR;
        meta[TOKEN]=0x300;  
      } 
      
      boolean isCompleted = chain[position].areWeThereYet(chain,position,_meta);
      
      if ( isCompleted )
      {
        if (_data!=null)
        {
          meta[LANGUAGE]=LANGUAGE_ERROR;
          meta[TOKEN]=0x301;  
        }
      }
      else
      {
        if (_data==null)
        {
          meta[LANGUAGE]=LANGUAGE_ERROR;
          meta[TOKEN]=0x302;  
        }
      }
    }
    
    protected void _processNext(int[] meta)
    {
      // specific processing here
      _offsetRead = _meta[OFFSET];
      _lengthRead = _meta[LENGTH];
      System.arraycopy(_meta,0,meta,0,meta.length);
    }
    
    protected void _sendOutputData(ITokyoNaut[] chain, int position)
    {
      if (position<chain.length)
        chain[position].notYet(this,_data);
    }
    
    public boolean areWeThereYet(ITokyoNaut[] chain, int position, int[] meta)
    {
      if ( _isInputEmpty() )
      {
        _loadInputData(chain,position-1,meta);
        
        if ( _isError(meta) || _isInputEmpty() )
        {
          _terminate();
          return true;
        }
      }
      
      _processNext(meta);
      
      if ( _isError(meta) )
      {
        _terminate();
        return true;
      }
      
      _sendOutputData(chain,position+1);
      return false;
    }
    
    public void notYet(ITokyoNaut source, byte[] data)
    {
      _data = data;
    }
    
  }
  
After initializing both classes with the above filtering code, changing the class name where appropriate
and making the error codes unique by changing the first digit, you can introduce these two TokyoNauts 
in the chain without side-effect since they behave as identity filters. You just have to add the 
corresponding import declarations and change the initialization code before the main loop: 

  // In ProtoOneMainLoop.java
  import net.sf.tokyo.prototype1.tokyonauts.UTF8ToUnicodeNaut;
  import net.sf.tokyo.prototype1.tokyonauts.UnicodeToUTF8Naut;
  
  (...)
  
  public static void main(String[] args)
  {
    (...)
    
    FileToBinaryNaut source = new FileToBinaryNaut(inFilePath);
    BinaryToFileNaut destination = new BinaryToFileNaut(outFilePath);
    
    UTF8ToUnicodeNaut utf8decoder = new UTF8ToUnicodeNaut();
    UnicodeToUTF8Naut utf8encoder = new UnicodeToUTF8Naut();
    
    ITokyoNaut[] chain = {source, utf8decoder, utf8encoder, destination};
    
    int[] meta = new int[7];
    
    (...) // main loop code quoted above
  }
  
After running the modified chain and checking that no changes occured, I then modified the identity
filter code above to introduce one at a time UTF-8 character conversion in the first filter:

  // In UTF8ToUnicodeNaut:
  (PASTE CODE HERE)
  
And I introduced the code below in the second filter to convert individual Unicode characters to UTF-8 bytes:
  
  // In UnicodeToUTF8Naut.java
  (PASTE CODE HERE)

You can check each class for the details of added imports, constructors and termination methods, as well
as detail of changes to the identity filter skeleton code.
  
Since data fragments are cut into pieces corresponding to one Java character or one Unicode character, but 
the overall data sequence is unchanged, the exact same file is produced in the output of BinaryToFileNaut, 
which you can check by looking at the CSV file produced in out/data/result.csv. The result is the same,
but we can already see that the analysis, reaching the character level, is more fine-grained.

Stage 4: Complete.


===================================================================================================
                      =#= Stage 5: CSV appears with the Fifth and Sixth TokyoNaut =#=
===================================================================================================






===================================================================================================
                      =#= Stage 6: XML appears with the Seventh and Eighth TokyoNaut =#=
===================================================================================================






===================================================================================================
                   =#= Stage 7: Nineth TokyoNaut briges XML-to-XML through XSLT =#=
===================================================================================================


  
You can notice the introduction of a new token, TOKEN_REMAIN, used to report some data part of an
incomplete token to the source, to be patched with more data with next delivery. The updated code
including management of TOKEN_REMAIN in FileToBinaryNaut now looks like:

  // In FileToBinaryNaut.java:
  public boolean areWeThereYet(int[] meta, byte[] data)
  {
    if ( super.areWeThereYet(meta,data) || _in==null )
      return true;
    
    int writeOffset;
    int writeLength;
    
    if (meta[TOKEN]==TOKEN_REMAIN)
    {
      writeOffset = meta[LENGTH];
      writeLength = data.length-writeOffset;
    }
    else
    {
      writeOffset = 0;
      writeLength = data.length;
    }
    
    try
    {
      if ( _in.available()==0 )
        return true;
      
      int bytesRead = _in.read(data,writeOffset,writeLength);
      
      meta[OFFSET] = 0;
      meta[LENGTH] = (bytesRead==-1? writeOffset : writeOffset+bytesRead);
      
      meta[LEFT] = (meta[TOKEN]==TOKEN_SPARK? LEFT_START: LEFT_CONTINUED);
      meta[RIGHT] = (_in.available()==0? RIGHT_END : RIGHT_CONTINUED);
      
      meta[LANGUAGE] = LANGUAGE_BINARY;
      meta[TOKEN] = TOKEN_BINARY;
    }
    catch(Exception e)
    {
      System.err.println("Error in FileToBinaryNaut#areWeThereYet(): "+e);
      meta[LANGUAGE]=LANGUAGE_ERROR;
      meta[TOKEN]=0x101;
      return true;
    }
    
    return false;
  }



===================================================================================================
                                        ~ The End ~
===================================================================================================

* FUTURE DIRECTION: AUTOMATION USING GRAMMAR DEFINITIONS
  Inspired by the example of compiler program generation based on grammar definition,
  I am quite convinced that the same could prove useful, given appropriate required 
  adaptations, to transform data, e.g. to get an XML view of binary data:
  
  - define the grammar of your data
  - define the vocabulary for your data viewed as XML
  - define the syntax of your queries, e.g. the required XPath subset
  - use them to create 
      - one tool reading your data and implementing an XML parsing API,
        with the following possible components:
        * Query parsing and analysis
        * State Machine for Data Grammar
        * State Machine/Factory for Data Extraction
        
      - one tool providing an XML serializing API and writing your data,
        with the following possible components:
        * Query parsing and analysis
        * State Machine for Data Grammar
        * State Machine/Factory for Data Serialization
      
      - and maybe additional tools to validate input data / generated data
      
  - plug your tool on applications processing XML

  NOTE:
    - you may need some adapters for the expected API or extra conversion steps
    - the application has to know the vocabulary of your data viewed as XML 
      to understand its semantic
  
  At the core lies the need to formalize the grammar of your data, which could be done with 
  several to be determined notations (EBNF, tables, encoding algorithm,...). I mean here that 
  the tool should be extensible to support several grammars, not be limitated to a single one.
  
  Adapted to handle binary data and not just text and character tokens, the grammar should define:
    - some structure, which will be mapped to the XML tree structure
    - some codecs, defining the format of leaf nodes
    - some comments, explaining the semantics of different fields
    - there may also be some additional relations between different elements,
    that must be understood for proper decoding (choice, repeat, based on values read before)
  
  This project may end up defining a general format good enough for 80% of grammar definitions,
  or using different grammar types for different formats.
        
=============== END OF DOCUMENT: Tokyo Project - Process ===============